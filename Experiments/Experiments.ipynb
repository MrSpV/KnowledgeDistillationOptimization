{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from models_scripts import i3_res50, i3_res50_nl\n",
    "from models_scripts import I3Res50\n",
    "from model_resnet18 import I3Res18, freeze_bn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from KD_Lib.KD import VanillaKD\n",
    "from KD_Lib.KD import DML\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from copy import deepcopy\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline\n",
    "from models_scripts import model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import pytorchvideo\n",
    "import json\n",
    "from dataset_scripts import CTDataset, CTDatasetTestSimple\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import urllib\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from utilities_scripts import SAM, LR_Scheduler, get_criterion, LoadingBar, Log, initialize, RandAugment\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "fold_train_path = \"./train_folds.json\"\n",
    "fold_test_path = \"./test_folds.json\"\n",
    "fold_id = \"1\"\n",
    "root = \"D://COVID-CTset\"\n",
    "clip_len = 128\n",
    "batch_size = 1\n",
    "fold_splitter_test = None\n",
    "fold_splitter_train = None\n",
    "    \n",
    "with open(fold_train_path) as fhandle:\n",
    "    fold_splitter_train = json.load(fhandle)\n",
    "    \n",
    "with open(fold_test_path) as fhandle:\n",
    "    fold_splitter_test = json.load(fhandle)\n",
    "    \n",
    "dataset_test = CTDatasetTestSimple(root=root, \n",
    "                      fold_id=fold_id, \n",
    "                      fold_splitter=fold_splitter_test,\n",
    "                      transforms=None,\n",
    "                      replacer=\"\",\n",
    "                      prepath=\"\",\n",
    "                      clip_len=clip_len,\n",
    "                      split=\"test\"\n",
    "                      )\n",
    "\n",
    "dataset_train = CTDatasetTestSimple(root=root, \n",
    "                      fold_id=fold_id, \n",
    "                      fold_splitter=fold_splitter_train,\n",
    "                      transforms=None,\n",
    "                      replacer=\"\",\n",
    "                      prepath=\"\",\n",
    "                      clip_len=clip_len,\n",
    "                      split=\"train\"\n",
    "                      )\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "\n",
    "teacher = i3_res50_nl(n_classes)\n",
    "student = I3Res18(num_classes = n_classes)\n",
    "\n",
    "teacher_pytorch_total_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_pytorch_total_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "teacher_layers = len(dict(teacher.named_modules()).keys())\n",
    "student_layers = len(dict(student.named_modules()).keys())\n",
    "\n",
    "print('Teacher total parameters: ', end='')\n",
    "print(teacher_pytorch_total_params)\n",
    "print('Teacher number of layers: ', end='')\n",
    "print(teacher_layers)\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "\n",
    "print('Student total parameters: ', end='')\n",
    "print(student_pytorch_total_params)\n",
    "print('Teacher number of layers: ', end='')\n",
    "print(student_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001, weight_decay=0.0005) \n",
    "optimizer3 = torch.optim.ASGD(teacher.parameters(), lr=0.001, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n",
    "optimizer2 = torch.optim.SGD\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, eps=1e-06)\n",
    "Adamax = torch.optim.Adamax(teacher.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.05\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.005\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer2 = SAM(teacher.parameters(), base_optimizer, rho=rho, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "optimizerS = torch.optim.Adam(student.parameters(), lr=0.001, weight_decay=0.0005) \n",
    "optimizerT = torch.optim.Adam(teacher.parameters(), lr=0.001, weight_decay=0.0005) \n",
    "\n",
    "teacher_model = deepcopy(teacher)\n",
    "student_model = deepcopy(student)\n",
    "teacher_optimizer = optimizer\n",
    "student_optimizer = optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "distiller = VanillaKD(teacher_model, student_model, dataloader_test, dataloader_test, \n",
    "                      optimizer2, optimizerS)  \n",
    "\n",
    "distiller.train_teacher(epochs=10, plot_losses=True, save_model=True)\n",
    "distiller.train_student(epochs=10, plot_losses=True, save_model=True)\n",
    "distiller.evaluate(teacher=False)\n",
    "distiller.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "print(torch.cuda.memory_summary(device=torch.device(\"cuda:0\"), abbreviated=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
